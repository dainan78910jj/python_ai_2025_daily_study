{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b782a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 2\n",
    "\n",
    "# 1, open a text file\n",
    "# 2, for loop to get each line \n",
    "        # run decoding model Beam search\n",
    "        # run decoding model Sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0bb0049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, infer_device\n",
    "import pandas\n",
    "\n",
    "# 可以将其理解为：\n",
    "#     AutoTokenizer 有一堆预先定义好的神经网络结构定义\n",
    "#     输入的‘google/flan-t5-base’是一个已经训练好的神经网络的参数表\n",
    "#     把参数表给进AutoTokenizer，AutoTokenizer会自动识别并选择一个匹配的网络定义，整合成一个可以使用的神经网络用于下一步处理\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c709b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beamSearchGenerator(inputs):\n",
    "    return model.generate(**inputs, max_new_tokens=50, num_beams=2)\n",
    "\n",
    "def sampleGenerator(inputs):\n",
    "    return model.generate(**inputs, do_sample=True, num_beams=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "204945a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rows paased constrains (bs):  0.9130434782608695\n",
      "% of rows paased constrains (s):  0.13043478260869565\n",
      "average tokens out:  37.78260869565217\n",
      "total:  23\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./article.txt\"\n",
    "\n",
    "data = {\n",
    "    \"input\": [],\n",
    "    'output': [],\n",
    "    'decoding':[],\n",
    "    'tokens_out':[],\n",
    "    'constraint_passed':[],\n",
    "    'notes':[]\n",
    "}\n",
    "\n",
    "df = pandas.DataFrame(data)\n",
    "\n",
    "def count(input):\n",
    "    return input['input_ids'].size(dim=1)\n",
    "\n",
    "def validator(input):\n",
    "    if not input[0].endswith(\".\"):\n",
    "        return 'No period'\n",
    "    if len(input[0]) < 10:\n",
    "        return 'Too short'\n",
    "    return True\n",
    "\n",
    "sum_passed_constraints_bs = 0\n",
    "sum_passed_constraints_s = 0\n",
    "avg_token = 0\n",
    "total = 0\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        line = line.rstrip(\"\\n\")  # remove trailing newline\n",
    "        if len(line) > 0:\n",
    "            \n",
    "            total += 1\n",
    "            \n",
    "            inputs = tokenizer(line, return_tensors=\"pt\")\n",
    "            \n",
    "            outputs_bs = beamSearchGenerator(inputs)\n",
    "            outputs_s = sampleGenerator(inputs)\n",
    "            \n",
    "            generate_bs = tokenizer.batch_decode(outputs_bs, skip_special_tokens=True)\n",
    "            generate_s = tokenizer.batch_decode(outputs_s, skip_special_tokens=True)\n",
    "            \n",
    "            # print(\"Line: \" + line)\n",
    "            # print(inputs)\n",
    "            # print(outputs_bs)\n",
    "            # print(generate_bs)\n",
    "            # print(outputs_s)\n",
    "            # print(generate_s)\n",
    "            \n",
    "            value = count(inputs)\n",
    "            \n",
    "            avg_token += value\n",
    "            \n",
    "            validation_bs = validator(generate_bs)\n",
    "            validation_s = validator(generate_s)\n",
    "            \n",
    "            validation_pass_bs = True if validation_bs == True else False\n",
    "            validation_pass_s = True if validation_s == True else False\n",
    "            \n",
    "            if validation_pass_bs == True:\n",
    "                sum_passed_constraints_bs += 1\n",
    "            if validation_pass_s == True:\n",
    "                sum_passed_constraints_s += 1\n",
    "            \n",
    "            notes_bs = \"\" if validation_bs == True else validation_bs\n",
    "            notes_s = \"\" if validation_s == True else validation_s\n",
    "            \n",
    "            df.loc[len(df)] = [\n",
    "                line, generate_bs[0], 'beam_search', value, validation_pass_bs, notes_bs\n",
    "            ]\n",
    "            \n",
    "            df.loc[len(df)] = [\n",
    "                line, generate_s[0], 'sampling', value, validation_pass_s, notes_s\n",
    "            ]\n",
    "            \n",
    "            \n",
    "df.to_csv('results.csv')\n",
    "\n",
    "avg_token /= total\n",
    "\n",
    "print('% of rows paased constrains (bs): ', sum_passed_constraints_bs / total)\n",
    "print('% of rows paased constrains (s): ', sum_passed_constraints_s / total)\n",
    "print('average tokens out: ', avg_token)\n",
    "print('total: ', total)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
